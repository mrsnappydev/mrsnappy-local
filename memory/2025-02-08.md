# 2025-02-08

## Session Start
- Context was lost due to compaction - rebuilt from project files and git history
- Set up memory system to prevent future context loss:
  - Created `MEMORY.md` with comprehensive project knowledge
  - Created `memory/` directory for daily notes
  - Added cron job: auto-save every 10 minutes (job id: `c921348d-0507-438a-a38f-e70367b819e7`)

## MrSnappy Local - Codebase Review
Completed full review of the codebase structure:
- Next.js 14 app in `apps/web/`
- Provider system: Ollama + LM Studio support via API proxies
- Model selection via `ModelHub.tsx` component
- Settings persisted to localStorage via `useSettings.ts`
- Recent work: provider polling fixes, custom Ollama URL, one-click install

## Current Issue
Paul reports: "issues with Ollama model selection"
- Need to get specific error/behavior details
- Possible areas: status API, model registry, selection handler, settings

## Completed
- [x] Fixed Central Storage → Ollama import bug
- [x] Committed: `d28a156` - auto-copy model to Ollama directory when path not accessible

## The Fix
When Ollama can't access the model file (running as systemd service with different user):
1. First tries normal import with original path
2. If fails with permission error, copies to `~/.ollama/models/mrsnappy-imports/`
3. Retries import with the copied path
4. Original file in Central Storage is preserved

Cross-platform: checks `OLLAMA_MODELS` env var, falls back to `~/.ollama/models/`

## Still TODO
- [ ] Intelligent model suggestions based on user input
- [ ] Test the fix on Paul's machine

---

## Auto-Save Checkpoints

**07:41 UTC** — Identified Central Storage → Ollama import bug. Root cause: Ollama service runs as different user, can't read ~/MrSnappy-Models/.

**07:51 UTC** — Paul confirmed fix approach: shared/accessible location, must work cross-platform (Windows, Mac, Linux, Docker).

**08:00 UTC** — Implemented fix: auto-copy to Ollama's models dir when original path not accessible. Committed `d28a156`.

**10:01 UTC** — Fix didn't work. Error still occurs. Issue: copied to wrong location (~/.ollama/models/ but systemd Ollama uses /usr/share/ollama/.ollama/models/). 

**10:10 UTC** — Improved fix: now tries multiple shared locations:
- Linux: /var/lib/mrsnappy/models → /tmp/mrsnappy-models (fallback)
- Sets 644 permissions on copied files (world-readable)
- Sets 755 on directories
- Committed `1fe55f8`

**10:21 UTC** — Got logs! Copy WAS working to `/tmp/mrsnappy-models/` but Ollama still couldn't see it.

**Root cause:** systemd `PrivateTmp=true` — Ollama has its own private `/tmp` directory!

**Fix:** Use `/var/tmp` instead, which is NOT affected by PrivateTmp. Committed `e983b7c`.

**10:31 UTC** — Disk full! Copy approach won't work with 100GB+ of models.

**New approach:** Try chmod first to make original file readable by Ollama:
1. chmod 755 on ~/MrSnappy-Models directory
2. chmod 644 on the model file
3. Retry import with original path
4. Only copy as last resort

Committed `9a170c8`.

**10:41 UTC** — chmod on file/dir not enough. Problem: `/home/paul` itself is likely 700 (private).
Ollama can't traverse through home directory to reach MrSnappy-Models.

**Fix:** Walk up ENTIRE path and add +x (traverse) permission to all parent directories.
Committed `cb64112`.

**11:01 UTC** — Still failing. App's chmod doesn't seem to work. 
Having Paul run manual chmod to test:
- `chmod a+x /home/paul`
- `chmod -R a+rX ~/MrSnappy-Models`
- Then verify with `sudo -u ollama head -c 100 <file>`

If manual chmod works but app chmod doesn't, need to investigate why.

**11:11 UTC** — Stopped chasing Ollama permission rabbit hole. Committed clear error message with fix instructions. `e692038`

**New direction:** Paul wants ME (Claude/Clawdbot) to be the **head agent** in MrSnappy:
- Local models (Ollama/LM Studio) don't reliably use tools
- I orchestrate and use tools (web search, etc.)
- Local models become workers for generation tasks
- Architecture: User → MrSnappy → Claude → Local LLMs + Tools

**Committed `5ff2f93`:** Full Claude/Anthropic provider implementation:
- New provider with streaming support
- API key settings in UI
- Routes through `/api/providers/anthropic/*`
- Head agent architecture ready

**12:20 UTC** — Added local_llm delegation tool:
- Claude can now delegate tasks to Ollama/LM Studio
- Fixed server-side fetch URL issue (calls Ollama directly now)
- Test: "Use Ollama to write a haiku" → Claude delegates to local model
- Commit `24fa93b`

