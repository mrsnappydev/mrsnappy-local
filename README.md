# MrSnappy Local ğŸŠâš¡

**Your private AI assistant that runs entirely on your machine.** No cloud, no subscriptions, no data leaving your device. Just you and a powerful AI, having a conversation.

<p align="center">
  <img src="docs/screenshot.png" alt="MrSnappy Local Screenshot" width="800">
</p>

## âœ¨ Why MrSnappy Local?

| Feature | MrSnappy Local | Cloud AI Services |
|---------|----------------|-------------------|
| ğŸ”’ **Privacy** | 100% local - data never leaves your device | Data sent to external servers |
| ğŸ’° **Cost** | Free forever | $20+/month subscriptions |
| âš¡ **Speed** | No internet latency | Depends on connection |
| ğŸ”Œ **Offline** | Works without internet | Requires internet |
| ğŸ›ï¸ **Control** | Choose any model, customize everything | Limited options |

---

## ğŸš€ Features

### Core Chat
- **ğŸ¦™ Multi-Provider Support** - Works with [Ollama](https://ollama.ai) and [LM Studio](https://lmstudio.ai)
- **ğŸ”„ One-Click Model Switching** - Switch between models instantly
- **âš¡ Streaming Responses** - Watch responses appear in real-time
- **ğŸ’¾ Persistent History** - Conversations saved locally
- **âœï¸ Edit & Regenerate** - Edit messages and regenerate responses
- **ğŸ¨ Beautiful UI** - Dark theme, syntax highlighting, markdown rendering

### ğŸ“¦ Central Model Storage
- **Download once, use everywhere** - One storage location for all models
- **Import from Ollama/LM Studio** - Consolidate existing models
- **Huggingface Downloads** - Browse and download GGUF models directly
- **Model Capabilities** - See what each model is good at (coding, vision, etc.)

### ğŸ§  Memory System
- **Remembers you** - Stores facts, preferences, and context
- **AI-powered extraction** - Auto-detect important info from chats
- **Full control** - Add, edit, delete memories manually

### ğŸ”§ Integrations
- **ğŸ” Web Search** - DuckDuckGo integration (no API key needed)
- **ğŸ–¼ï¸ Image Search** - Visual results with thumbnails
- **ğŸ“§ Gmail** - Read, send, search emails (OAuth)
- **ğŸ“… Calendar** - View and manage Google Calendar events

### ğŸ“ Project Workspaces
- **Organize your work** - Create named projects with dedicated folders
- **Safety prompts** - Confirms which project you're working on
- **File tools** - Create, read, list files in project context

### ğŸ’» System Monitoring
- **Resource widget** - Shows RAM, CPU, GPU/VRAM usage
- **Model recommendations** - "Best for coding", "Best for low RAM"
- **Thinking indicator** - See when the AI is processing

---

## ğŸ“¦ Central Model Storage â€” Deep Dive

### The Problem

Without central storage, you end up with:
- **Duplicate models** - Same model downloaded in both Ollama and LM Studio
- **Wasted disk space** - 7B models are 4-8GB each!
- **No organization** - Models scattered across different directories
- **Provider lock-in** - Models stuck in one provider's format

### Our Solution

MrSnappy uses a **Central Model Storage** system:

```
~/MrSnappy-Models/
â”œâ”€â”€ .registry.json          # Tracks all models and their metadata
â”œâ”€â”€ llama-3.2-8b-q4.gguf   # Actual model files
â”œâ”€â”€ mistral-7b-q5.gguf
â”œâ”€â”€ codellama-13b-q4.gguf
â””â”€â”€ ...
```

**Benefits:**
- âœ… **Download once** - Model lives in one place
- âœ… **Use with any provider** - Import to Ollama, LM Studio, or both
- âœ… **Save disk space** - No duplicates (uses symlinks)
- âœ… **Easy backup** - One folder to backup/migrate
- âœ… **Metadata tracking** - Know where each model came from, its capabilities, etc.

### How It Works

#### 1. Downloading New Models

1. Open **Model Hub** (click model name in header)
2. Go to **Huggingface** tab
3. Search for a model (e.g., "llama 3.2")
4. Click **Download** â†’ saved to central storage
5. Click **Import to Ollama** or **Import to LM Studio**

#### 2. Importing Existing Models

Already have models in Ollama or LM Studio? Import them!

1. Open **Model Hub** â†’ **Central Storage** tab
2. Click **Import Existing** tab
3. Click **Scan for Models**
4. Select models to import
5. Optionally check "Delete original after import"
6. Click **Import Selected**

#### 3. Configure Providers to Use Central Storage

Want Ollama/LM Studio to use central storage even without MrSnappy?

1. Open **Model Hub** â†’ **Central Storage** â†’ **Configure Storage** tab
2. Click **Configure Ollama** or **Configure LM Studio**
3. This creates symlinks so providers read from central storage
4. Original directories are backed up first

### Model Capabilities

Each model shows capability badges:

| Badge | Meaning |
|-------|---------|
| ğŸ’» Coding | Good at code generation |
| ğŸ‘ï¸ Vision | Can understand images |
| âœï¸ Creative | Good at creative writing |
| ğŸ§® Reasoning | Good at logic/math |
| âš¡ Fast | Optimized for speed |
| ğŸ“± Small | Runs on limited hardware |
| ğŸŒ Multilingual | Multiple languages |
| ğŸ”“ Uncensored | Fewer content restrictions |

**Filter by capability** in Model Hub to find the right model for your task!

### Recommended Models by Task

| Task | Recommended Models | Why |
|------|-------------------|-----|
| **General Chat** | Llama 3.2, Mistral | Good all-rounders |
| **Coding** | CodeLlama, DeepSeek-Coder, Qwen-Coder | Trained on code |
| **Low RAM (<8GB)** | Phi-3, Gemma-2-2B, TinyLlama | Small but capable |
| **Image Understanding** | LLaVA, BakLLaVA, Moondream | Vision capabilities |
| **Creative Writing** | Mistral, Nous-Hermes | Good at storytelling |
| **Technical/Math** | WizardMath, DeepSeek | Reasoning-focused |

---

## ğŸ“‹ Quick Start

### Prerequisites

You need **one** of the following AI backends:

**Option A: Ollama** (Recommended)
```bash
# Linux/Mac
curl -fsSL https://ollama.ai/install.sh | sh

# Then start it:
ollama serve

# Pull a model:
ollama pull llama3.2
```

**Option B: LM Studio**
- Download from [lmstudio.ai](https://lmstudio.ai)
- Open â†’ Discover â†’ Download a model
- Local Server â†’ Start Server

### Installation

```bash
# Clone the repository
git clone https://github.com/mrsnappydev/mrsnappy-local.git
cd mrsnappy-local

# Install dependencies
npm install

# Start the development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) and start chatting! ğŸ‰

---

## ğŸ’» System Requirements

### Minimum
- 8GB RAM (for small models)
- 5GB free disk space
- Modern CPU (2018+)

### Recommended
- 16GB+ RAM
- NVIDIA GPU with 8GB+ VRAM (or Apple Silicon)
- SSD for fast model loading

### Model Size Guide

| Model Size | RAM Needed | GPU VRAM | Examples |
|------------|------------|----------|----------|
| ~2B params | 4-6 GB | 4 GB | Phi-3, Gemma-2-2B |
| ~7B params | 8-12 GB | 6-8 GB | Llama 3.2, Mistral 7B |
| ~13B params | 16+ GB | 10+ GB | CodeLlama 13B |
| ~70B params | 48+ GB | 40+ GB | Llama 3.1 70B |

---

## ğŸ—ï¸ Architecture

```
mrsnappy-local/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ web/                      # Next.js web interface
â”‚       â””â”€â”€ src/
â”‚           â”œâ”€â”€ app/              # Routes & API endpoints
â”‚           â”‚   â””â”€â”€ api/
â”‚           â”‚       â”œâ”€â”€ auth/     # OAuth (Gmail, Calendar)
â”‚           â”‚       â”œâ”€â”€ chat/     # Chat endpoints
â”‚           â”‚       â”œâ”€â”€ models/   # Model management
â”‚           â”‚       â”œâ”€â”€ providers/# Provider proxies
â”‚           â”‚       â”œâ”€â”€ projects/ # Project management
â”‚           â”‚       â”œâ”€â”€ system/   # System stats
â”‚           â”‚       â””â”€â”€ tools/    # Tool execution
â”‚           â”œâ”€â”€ components/       # UI components
â”‚           â”‚   â”œâ”€â”€ ModelHub.tsx
â”‚           â”‚   â”œâ”€â”€ ModelStorage.tsx
â”‚           â”‚   â”œâ”€â”€ ProjectSelector.tsx
â”‚           â”‚   â”œâ”€â”€ MemoryPanel.tsx
â”‚           â”‚   â””â”€â”€ ...
â”‚           â”œâ”€â”€ hooks/            # React hooks
â”‚           â”œâ”€â”€ lib/
â”‚           â”‚   â”œâ”€â”€ providers/    # Ollama, LM Studio adapters
â”‚           â”‚   â”œâ”€â”€ integrations/ # Gmail, Calendar clients
â”‚           â”‚   â”œâ”€â”€ models/       # Model detection, storage
â”‚           â”‚   â”œâ”€â”€ tools/        # Tool definitions
â”‚           â”‚   â””â”€â”€ projects/     # Project types
â”‚           â””â”€â”€ types/            # TypeScript types
â””â”€â”€ packages/                     # Shared packages (planned)
```

### Key Design Decisions

**1. Server-Side API Proxies**
- All provider calls go through Next.js API routes
- Avoids CORS issues (browser â†’ Next.js â†’ Ollama)
- Users don't need to configure `OLLAMA_ORIGINS`

**2. Provider Abstraction**
```typescript
interface ModelProvider {
  checkConnection(): Promise<boolean>;
  getModels(): Promise<ModelInfo[]>;
  chat(request: ChatRequest): Promise<ChatResponse>;
  chatStream(request: ChatRequest): Promise<ReadableStream>;
}
```

**3. Tool Framework**
- Tools defined with JSON schema parameters
- LLM outputs `<tool_call>` tags when it wants to act
- Server executes tools and returns results
- Extensible â€” add new tools by implementing the interface

**4. Central Storage Registry**
- JSON file tracks all models with metadata
- Symlinks to share models between providers
- Survives provider reinstalls

---

## âš™ï¸ Configuration

### Environment Variables

Create `.env.local` in `apps/web/`:

```bash
# Provider URLs (usually not needed - defaults work)
OLLAMA_URL=http://localhost:11434
LMSTUDIO_URL=http://localhost:1234

# Central storage path (optional)
MODEL_STORAGE_PATH=~/MrSnappy-Models

# Gmail OAuth (optional - for email integration)
GMAIL_CLIENT_ID=your_client_id
GMAIL_CLIENT_SECRET=your_client_secret
```

All settings can also be configured through the UI Settings panel.

---

## ğŸ—ºï¸ Roadmap

### âœ… Completed
- [x] Core chat interface with streaming
- [x] Ollama + LM Studio support
- [x] Conversation persistence & search
- [x] Edit/regenerate messages
- [x] **Central Model Storage**
- [x] **Import from existing providers**
- [x] **Model capabilities & recommendations**
- [x] Huggingface model browser & downloads
- [x] Web search (DuckDuckGo)
- [x] Image search with visual results
- [x] Gmail integration (OAuth)
- [x] Calendar integration (OAuth)
- [x] Project workspaces
- [x] Memory system (remember facts)
- [x] System stats widget (RAM/CPU/GPU)
- [x] User onboarding & personalization
- [x] Built-in help system

### ğŸ“‹ Planned
- [ ] Voice input/output
- [ ] Desktop app (Tauri/Electron)
- [ ] Mobile companion apps
- [ ] One-click installer
- [ ] Docker support
- [ ] Plugin system

---

## ğŸ› Troubleshooting

### "No AI Provider Running"
```bash
# Start Ollama:
ollama serve

# Or start LM Studio:
# Open app â†’ Local Server â†’ Start Server
```

### Models Not Detected in Import
Check the paths shown in the Import UI. If your provider stores models elsewhere:
```bash
# Find Ollama models:
ls -la ~/.ollama/models/

# Find LM Studio models:
ls -la ~/.lmstudio/models/
ls -la ~/.cache/lm-studio/models/
```

### Slow Responses
- Try a smaller model (phi3, gemma-2-2b)
- Check system stats widget for resource usage
- Enable streaming for perceived speed

### Gmail/Calendar Not Connecting
- Make sure you've set up Google Cloud OAuth credentials
- Check the Help guide (? icon) for step-by-step instructions

---

## ğŸ¤ Contributing

Contributions welcome!

```bash
# Fork & clone
git clone https://github.com/YOUR_USERNAME/mrsnappy-local.git

# Create feature branch
git checkout -b feature/amazing-feature

# Make changes & test
npm run dev
npm run build

# Commit & push
git commit -m 'Add amazing feature'
git push origin feature/amazing-feature

# Open Pull Request
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

<p align="center">
  <strong>Built with âš¡ by <a href="https://torsbotech.com">Torsbotech</a></strong>
  <br>
  <em>Paul & MrSnappy</em>
</p>

<p align="center">
  <a href="https://github.com/mrsnappydev/mrsnappy-local/issues">Report Bug</a>
  Â·
  <a href="https://github.com/mrsnappydev/mrsnappy-local/issues">Request Feature</a>
</p>
