# MrSnappy Local âš¡

A local AI assistant that runs entirely on your machine. Private, fast, and always available.

## Features

- ğŸ  **100% Local** â€” All processing happens on your machine
- ğŸ”’ **Privacy-First** â€” Your conversations never leave your device
- âš¡ **Fast** â€” No internet latency
- ğŸ”Œ **Multi-Provider** â€” Works with Ollama and LM Studio
- ğŸ¨ **Beautiful UI** â€” Clean, modern chat interface
- ğŸ“¦ **Easy Setup** â€” One-click installation (coming soon)

## Supported Backends

| Provider | API Type | Default Port | Status |
|----------|----------|--------------|--------|
| ğŸ¦™ [Ollama](https://ollama.ai) | Native Ollama API | 11434 | âœ… Supported |
| ğŸ›ï¸ [LM Studio](https://lmstudio.ai) | OpenAI-compatible | 1234 | âœ… Supported |

Both providers are auto-detected on startup. Switch between them seamlessly in Settings.

## Prerequisites

**Option A: Ollama**
- [Ollama](https://ollama.ai) installed and running
- A model pulled (e.g., `ollama pull llama3.2`)

**Option B: LM Studio**
- [LM Studio](https://lmstudio.ai) installed
- A model loaded and the local server started

## Development

```bash
# Install dependencies
npm install

# Start the web app
npm run dev

# Open http://localhost:3000
```

## Architecture

```
mrsnappy-local/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/                    # Next.js web interface
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ app/            # Routes & API endpoints
â”‚   â”‚       â”œâ”€â”€ components/     # UI components
â”‚   â”‚       â”œâ”€â”€ hooks/          # React hooks
â”‚   â”‚       â”œâ”€â”€ lib/
â”‚   â”‚       â”‚   â””â”€â”€ providers/  # Model provider abstraction
â”‚   â”‚       â””â”€â”€ types/          # TypeScript types
â”‚   â””â”€â”€ desktop/                # Tauri desktop wrapper (planned)
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ core/                   # Shared logic (planned)
â””â”€â”€ README.md
```

## Provider Abstraction

MrSnappy uses a provider abstraction layer to support multiple LLM backends:

```typescript
// lib/providers/types.ts
interface ModelProvider {
  checkConnection(): Promise<boolean>;
  getModels(): Promise<ModelInfo[]>;
  chat(request: ChatRequest): Promise<ChatResponse>;
  chatStream(request: ChatRequest): Promise<ReadableStream>;
}
```

Adding new providers (like LocalAI, vLLM, etc.) is straightforward â€” just implement the `ModelProvider` interface.

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_URL` | `http://localhost:11434` | Ollama API endpoint |
| `OLLAMA_MODEL` | `llama3.2` | Default model to use |

## Roadmap

### âœ… Phase 0: Foundation
- [x] Basic chat UI
- [x] Ollama integration
- [x] Streaming responses âš¡
- [x] Conversation history persistence ğŸ’¾
- [x] Settings panel âš™ï¸
- [x] Search across conversations ğŸ”
- [x] Export/Import conversations ğŸ“¤ğŸ“¥
- [x] Markdown rendering âœ¨
- [x] Code syntax highlighting ğŸ¨
- [x] Edit/regenerate messages âœï¸

### âœ… Phase 1: Model Flexibility
- [x] Provider abstraction layer ğŸ”Œ
- [x] LM Studio support (OpenAI-compatible) ğŸ›ï¸
- [x] Auto-detect available providers âš¡
- [x] Backend selector in Settings UI
- [ ] Central model folder (unified storage)
- [ ] Download models from Huggingface
- [ ] Model compatibility matrix

### ğŸ“‹ Phase 2: Full Capabilities
- [ ] Email integration (Gmail, etc)
- [ ] Calendar integration
- [ ] Web search
- [ ] File management

### ğŸ“‹ Phase 3: User-Friendly Setup
- [ ] Guided onboarding wizard
- [ ] Plain-language setup questions
- [ ] Automatic OAuth flows

### ğŸ“‹ Phase 4: 1-Click Deploy
- [ ] Single installer/script
- [ ] Docker compose
- [ ] Windows/Mac/Linux support

## Built by

**Torsbotech** â€” Paul & MrSnappy âš¡

## License

MIT
